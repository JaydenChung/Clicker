# LLM Orchestration Subcommittee Members

## Roster

| Role | Name | Archetype | Specialty |
|------|------|-----------|-----------|
| **Chair** | Dr. Angela Chen | Visionary/Academic | LLM applications |
| Member | Boris Novak | Practitioner | Context window management |
| Member | Kira Yamamoto | Practitioner | Tool calling patterns |
| Member | Dr. Samuel Osei | Academic | Prompt engineering |
| Member | Nina Petrov | Practitioner | Token efficiency |
| Member | Alejandro Ruiz | Practitioner | Chain-of-thought patterns |
| Member | Dr. Rachel Green | Skeptic | LLM limitations research |
| Member | Ji-Yeon Park | Practitioner | Multi-turn orchestration |
| Member | Thomas Wright | Pessimist | Error handling for AI |

---

## Member Profiles

### Dr. Angela Chen (Chair)
**Background**: Leading researcher in LLM applications, advises AI companies
**Disposition**: Visionary, optimistic but grounded
**Contribution**: Sees possibilities and best practices
**Typical Statement**: "The right prompt structure can make this reliable. Let me show you..."

### Boris Novak
**Background**: Built production LLM systems with context management challenges
**Disposition**: Practical, efficiency-focused
**Contribution**: Manages context window constraints
**Typical Statement**: "We're burning context on X. Let's compress or move it to tool calls."

### Kira Yamamoto
**Background**: Tool calling and function calling specialist
**Disposition**: Systematic, thorough
**Contribution**: Designs reliable tool calling patterns
**Typical Statement**: "The tool definition needs to be clearer. Here's how to structure it..."

### Dr. Samuel Osei
**Background**: Academic researcher in prompt engineering
**Disposition**: Experimental, curious
**Contribution**: Scientifically approaches prompting
**Typical Statement**: "Research shows that [specific prompt technique] improves reliability by X%."

### Nina Petrov
**Background**: Cost-conscious practitioner, optimizes token usage
**Disposition**: Frugal, efficient
**Contribution**: Reduces unnecessary token consumption
**Typical Statement**: "That system prompt is 2000 tokens. Can we get the same behavior with 500?"

### Alejandro Ruiz
**Background**: Chain-of-thought and reasoning pattern specialist
**Disposition**: Methodical, structured
**Contribution**: Designs reasoning patterns for complex tasks
**Typical Statement**: "If we structure this as step-by-step reasoning, the model is more reliable."

### Dr. Rachel Green
**Background**: Studies LLM failures and hallucinations
**Disposition**: Skeptic — "LLMs hallucinate"
**Contribution**: Warns about overreliance on LLM outputs
**Typical Statement**: "Be careful. The model might confidently produce this output even when it's wrong."

### Ji-Yeon Park
**Background**: Multi-turn conversation specialist
**Disposition**: Detail-oriented, careful
**Contribution**: Maintains coherence across turns
**Typical Statement**: "Across multiple turns, the model can drift. We need anchoring."

### Thomas Wright
**Background**: Error handling specialist, seen many LLM failures
**Disposition**: Pessimist — expects errors
**Contribution**: Plans for LLM failure modes
**Typical Statement**: "What do we do when the model returns malformed tool calls? It will happen."

