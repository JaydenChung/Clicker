# Multi-Agent Pipeline Architecture

> **Solves**: Context degradation across multiple applications
> **Enables**: Consistent quality resume tailoring at scale

---

## The Problem

With a single LLM context handling everything:

```
Application 1: Fresh context â†’ HIGH QUALITY tailoring â­â­â­â­â­
Application 2: Context bloated â†’ GOOD tailoring â­â­â­â­
Application 3: Context full â†’ DEGRADED tailoring â­â­â­
Application 4: Context exhausted â†’ POOR tailoring â­â­
```

The LLM "forgets" the tailoring instructions because they're buried under application history.

---

## The Solution: Fresh Context Per Phase

Each phase of the pipeline uses a FRESH LLM context:

```
Application 1:
  JD Analysis â†’ Fresh API call â†’ Perfect analysis â­â­â­â­â­
  Tailoring   â†’ Fresh API call â†’ Perfect resume â­â­â­â­â­
  Scoring     â†’ Fresh API call â†’ Accurate score â­â­â­â­â­

Application 10:
  JD Analysis â†’ Fresh API call â†’ Perfect analysis â­â­â­â­â­
  Tailoring   â†’ Fresh API call â†’ Perfect resume â­â­â­â­â­
  Scoring     â†’ Fresh API call â†’ Accurate score â­â­â­â­â­
```

**Same quality on application 1, 10, or 100.**

---

## Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         MULTI-AGENT PIPELINE                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                      â”‚
â”‚   CURSOR AGENT (Browser)                ORCHESTRATOR (Python - Background)           â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚   â”‚                         â”‚           â”‚                                         â”‚ â”‚
â”‚   â”‚  1. Find job listing    â”‚           â”‚   scripts/pipeline_orchestrator.py      â”‚ â”‚
â”‚   â”‚                         â”‚           â”‚                                         â”‚ â”‚
â”‚   â”‚  2. Extract JD text     â”‚           â”‚   Watches for: pending_jd.json          â”‚ â”‚
â”‚   â”‚     â†“                   â”‚           â”‚                                         â”‚ â”‚
â”‚   â”‚  3. Write pending_jd â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   4. JD Analysis (Fresh LLM call)       â”‚ â”‚
â”‚   â”‚                         â”‚           â”‚      â†“                                  â”‚ â”‚
â”‚   â”‚  4. WAIT...             â”‚           â”‚   5. Resume Tailoring (Fresh LLM call)  â”‚ â”‚
â”‚   â”‚     â†“                   â”‚           â”‚      â†“                                  â”‚ â”‚
â”‚   â”‚  5. Poll for ready      â”‚           â”‚   6. ATS Scoring (Fresh LLM call)       â”‚ â”‚
â”‚   â”‚     â†“                   â”‚           â”‚      â†“                                  â”‚ â”‚
â”‚   â”‚  6. Read resume_ready â—€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   7. Refinement loop (if score < 90%)   â”‚ â”‚
â”‚   â”‚     â†“                   â”‚           â”‚      â†“                                  â”‚ â”‚
â”‚   â”‚  7. Apply to job        â”‚           â”‚   8. Compile PDF                        â”‚ â”‚
â”‚   â”‚     â†“                   â”‚           â”‚      â†“                                  â”‚ â”‚
â”‚   â”‚  8. Log everything      â”‚           â”‚   9. Write resume_ready.json            â”‚ â”‚
â”‚   â”‚     â†“                   â”‚           â”‚                                         â”‚ â”‚
â”‚   â”‚  9. Next job            â”‚           â”‚   10. Wait for next job                 â”‚ â”‚
â”‚   â”‚                         â”‚           â”‚                                         â”‚ â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚              â”‚                                          â”‚                            â”‚
â”‚              â”‚        FILE-BASED COMMUNICATION          â”‚                            â”‚
â”‚              â”‚                                          â”‚                            â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â–¶ data/pipeline/ â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                        â”œâ”€â”€ pending_jd.json                                           â”‚
â”‚                        â”œâ”€â”€ resume_ready.json                                         â”‚
â”‚                        â””â”€â”€ orchestrator_status.json                                  â”‚
â”‚                                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Components

### 1. Cursor Agent (`/apply-pipeline`)

**Location**: `.cursor/commands/apply-pipeline.md`

**Responsibilities**:
- Browser automation (navigate, click, fill forms)
- Extract job descriptions
- Signal orchestrator (write `pending_jd.json`)
- Wait for tailored resume
- Apply to jobs
- Log events

**Does NOT**:
- Tailor resumes
- Score resumes
- Make heavy LLM calls

### 2. Python Orchestrator

**Location**: `scripts/pipeline_orchestrator.py`

**Responsibilities**:
- Watch for incoming JDs
- Call specialized agent modules with FRESH contexts
- Coordinate the tailor â†’ score â†’ refine loop
- Compile LaTeX to PDF
- Signal completion

**Agent Modules**:
- `scripts/agents/jd_analyzer.py` - Extract keywords and requirements
- `scripts/agents/resume_tailor.py` - Generate tailored LaTeX
- `scripts/agents/ats_scorer.py` - Score and provide feedback

### 3. File-Based Communication

**Why files?**
- Simple, reliable, cross-process
- Atomic writes (single line = safe)
- Easy to debug (human-readable)
- No complex IPC needed

**Files**:
| File | Writer | Reader | Purpose |
|------|--------|--------|---------|
| `pending_jd.json` | Cursor | Orchestrator | New job to process |
| `resume_ready.json` | Orchestrator | Cursor | Tailored resume ready |
| `orchestrator_status.json` | Orchestrator | Cursor | Progress updates |

---

## Data Flow

### Per-Job Flow

```
1. Cursor: Find job â†’ Extract JD
                         â†“
2. Cursor: Write data/pipeline/pending_jd.json
                         â†“
3. Orchestrator: Detect pending_jd.json
                         â†“
4. Orchestrator: analyze_jd() â†’ FRESH Claude API call
                         â†“
5. Orchestrator: tailor_resume() â†’ FRESH Claude API call
                         â†“
6. Orchestrator: score_resume() â†’ FRESH Claude API call
                         â†“
7. Orchestrator: if score < 90%:
                    - Get suggestions
                    - tailor_resume() again â†’ FRESH call
                    - score_resume() again â†’ FRESH call
                    - Repeat max 3 times
                         â†“
8. Orchestrator: pdflatex â†’ Compile PDF
                         â†“
9. Orchestrator: Write data/pipeline/resume_ready.json
                         â†“
10. Orchestrator: Delete pending_jd.json
                         â†“
11. Cursor: Detect resume_ready.json
                         â†“
12. Cursor: Read result, apply to job
                         â†“
13. Cursor: Delete resume_ready.json, move to next job
```

### Session Flow

```
Terminal 1:                              Terminal 2 (Cursor):
$ python3 scripts/pipeline_orchestrator.py
ğŸš€ Pipeline Orchestrator Started
Waiting for jobs...                      User: /apply-pipeline
                                         
ğŸ“¥ New JD detected: Google              â†’ Extracting JD...
  ğŸ“‹ Analyzing JD...                     â†’ Writing pending_jd.json
  âœï¸  Tailoring resume (iter 1)...       â†’ Waiting...
  ğŸ“Š Scoring (78%)...                    
  âœï¸  Tailoring resume (iter 2)...       
  ğŸ“Š Scoring (94%)...                    
  ğŸ“„ Compiling PDF...                    
âœ… Resume ready: 94%                     â†’ Found resume_ready.json
                                         â†’ Applying to job...
Waiting for next job...                  â†’ Moving to next job...
```

---

## Configuration

### Prerequisites

1. **Install LaTeX**:
   ```bash
   brew install basictex
   # Add to PATH: export PATH="/Library/TeX/texbin:$PATH"
   ```

2. **Install Python dependencies**:
   ```bash
   pip install anthropic
   ```

3. **Set API key**:
   ```bash
   export ANTHROPIC_API_KEY="your-key-here"
   ```

### Settings

| Setting | Location | Default |
|---------|----------|---------|
| Target ATS score | `pipeline_orchestrator.py` | 90% |
| Max refinement iterations | `pipeline_orchestrator.py` | 3 |
| Wait timeout | `apply-pipeline.md` | 5 minutes |

---

## Output Structure

```
resume/tailored/
â”œâ”€â”€ 20260119_103000_Google_SWE_final.pdf        # Ready to upload!
â”œâ”€â”€ 20260119_103000_Google_SWE_final.tex        # LaTeX source
â”œâ”€â”€ 20260119_103000_Google_SWE_jd_analysis.json # JD analysis
â”œâ”€â”€ 20260119_103000_Google_SWE_v1.tex           # Iteration 1
â”œâ”€â”€ 20260119_103000_Google_SWE_v1_score.json    # Score: 78%
â”œâ”€â”€ 20260119_103000_Google_SWE_v2.tex           # Iteration 2  
â”œâ”€â”€ 20260119_103000_Google_SWE_v2_score.json    # Score: 94% âœ“
â””â”€â”€ README.md                                    # Instructions
```

---

## Logging

### Cursor Events
Location: `data/events/session_YYYY-MM-DD_N.jsonl`

Events: `jd_extracted`, `tailoring_started`, `tailoring_completed`, `pending_upload`

### Orchestrator Logs
Location: `logs/pipeline/pipeline_YYYY-MM-DD.jsonl`

Events: `pipeline_started`, `jd_analyzed`, `resume_tailored`, `resume_scored`, `pipeline_completed`

---

## Handling File Upload Blocker

External sites can't receive programmatic file uploads (browser security).

**Solution**: Leave tab open, continue session

```
Session end output:

PENDING MANUAL UPLOAD (tabs left open):
â€¢ Stripe - External site
  Resume at: resume/tailored/..._Stripe_PM_final.pdf
  Tab should be open, just upload and submit
```

Human can quickly:
1. Find the open tab
2. Upload the tailored PDF
3. Submit

---

## Quick Start

### 1. Start the orchestrator (Terminal 1)
```bash
cd /Users/jchung/code/Clicker
export ANTHROPIC_API_KEY="your-key"
python3 scripts/pipeline_orchestrator.py
```

### 2. Run the Cursor command
In Cursor, type: `/apply-pipeline`

### 3. Watch the magic
- Cursor extracts JDs
- Orchestrator tailors resumes with fresh contexts
- Applications get submitted with 90%+ ATS scores
- PDFs saved for manual uploads

---

## Why This Architecture?

| Aspect | Single Agent | Pipeline |
|--------|--------------|----------|
| Context per job | Grows with history | Fresh each time |
| Quality consistency | Degrades | Constant |
| Debugging | Hard (mixed concerns) | Easy (isolated) |
| Scalability | Limited by context | Unlimited |
| Token efficiency | Wasted on history | Optimized |

---

## Future Enhancements

1. **Real-time dashboard**: Watch orchestrator progress in browser
2. **Parallel tailoring**: Tailor next resume while applying to current
3. **Resume caching**: Reuse similar tailorings for similar JDs
4. **External ATS APIs**: Use Jobscan-like services for higher accuracy
5. **Multi-model**: Use different models for different tasks

